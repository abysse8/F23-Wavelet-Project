{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the sparse representation and testing with multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "import pywt\n",
    "from wavelet_preprocessing import *\n",
    "\n",
    "dataset_path = \"../data\"\n",
    "batch_size = 32\n",
    "toTensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "train_data = datasets.FashionMNIST(dataset_path, train=True, transform=toTensor, download=True)\n",
    "test_data = datasets.FashionMNIST(dataset_path, train=False, transform=toTensor, download=True)\n",
    "sample_data = datasets.FashionMNIST(dataset_path, train=True, transform=None, download=True)\n",
    "label_names = test_data.classes\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size, shuffle=True)\n",
    "\n",
    "PERCENTILE_THRESH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_dwt(dataset, id):\n",
    "    img, label = dataset[id]\n",
    "    img = np.array(img) # grayscale images\n",
    "    details = dec_single_level_gray_combined(img, PERCENTILE_THRESH) # zeros out anything lower than 99th percentile\n",
    "    details /= np.max(details)\n",
    "    nz_row, nz_col = np.nonzero(details)\n",
    "    print(nz_row.shape[0])\n",
    "    vec = np.zeros((nz_row.shape[0], 3))\n",
    "    count = 0\n",
    "    for idx, i in enumerate(nz_row):\n",
    "        j = nz_col[idx]\n",
    "        vec[count, 0] = details[i,j]\n",
    "        vec[count, 1] = i / details.shape[0] # normalize row and col coords between [0, 1]\n",
    "        vec[count, 2] = j / details.shape[1]\n",
    "        count += 1\n",
    "    # sort by highest value\n",
    "    vec = vec[abs(vec[:, 0]).argsort()[::-1]]\n",
    "    vec = vec.ravel()\n",
    "    if vec.shape[0] < VEC_LEN:\n",
    "        diff = VEC_LEN - vec.shape[0]\n",
    "        zeros = np.zeros((diff,))\n",
    "        vec = np.concatenate([vec, zeros]) # pad zeros to ensure\n",
    "    else:\n",
    "        vec = vec[:VEC_LEN]\n",
    "    vec = torch.Tensor(vec)\n",
    "    return vec, label\n",
    "\n",
    "def preprocess_dwt(x, vec_len, thresh):\n",
    "    x_ = torch.zeros((x.shape[0], vec_len))\n",
    "    arr = x.squeeze().cpu().data.numpy()\n",
    "    for batch, img in enumerate(arr):\n",
    "        details = dec_single_level_gray_combined(img, thresh) # zeros out anything lower than percentile\n",
    "        details /= np.max(details)\n",
    "        nz_row, nz_col = np.nonzero(details)\n",
    "        vec = np.zeros((nz_row.shape[0], 3))\n",
    "        count = 0\n",
    "        for idx, i in enumerate(nz_row):\n",
    "            j = nz_col[idx]\n",
    "            vec[count, 0] = details[i,j]\n",
    "            vec[count, 1] = i / details.shape[0] # normalize row and col coords between [0, 1]\n",
    "            vec[count, 2] = j / details.shape[1]\n",
    "            count += 1\n",
    "        # sort by highest value\n",
    "        vec = vec[abs(vec[:, 0]).argsort()[::-1]]\n",
    "        vec = vec.ravel()\n",
    "        if vec.shape[0] < vec_len:\n",
    "            diff = vec_len - vec.shape[0]\n",
    "            zeros = np.zeros((diff,))\n",
    "            vec = np.concatenate([vec, zeros]) # pad zeros to ensure\n",
    "        else:\n",
    "            vec = vec[:vec_len]\n",
    "        x_[batch, :] = torch.Tensor(vec) # add coefficients to output tensor\n",
    "    return x_\n",
    "\n",
    "def get_sample_normal(dataset, id):\n",
    "    img, label = dataset[id]\n",
    "    img = np.array(img) # grayscale images\n",
    "    img = img.ravel()\n",
    "    vec = torch.Tensor(img)\n",
    "    return vec, label\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "    \n",
    "\n",
    "normal_loss_fn = nn.CrossEntropyLoss()\n",
    "dwt_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               dwt = False,\n",
    "               dwt_len = 50,\n",
    "               thresh = 90):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        if dwt:\n",
    "            X = preprocess_dwt(X, dwt_len, thresh)\n",
    "        # 1. Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "        train_acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              dwt = False,\n",
    "              dwt_len = 50,\n",
    "              thresh = 90):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            if dwt:\n",
    "                X = preprocess_dwt(X, dwt_len, thresh)\n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5,\n",
    "          dwt = False,\n",
    "          dwt_len = 100,\n",
    "          thresh = 90,\n",
    "          save: bool = False,\n",
    "          save_freq: int = 100):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "                \"test_loss\": [],\n",
    "                \"train_acc\": [],\n",
    "                 \"test_acc\": []}\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                dataloader=train_dataloader,\n",
    "                                loss_fn=loss_fn,\n",
    "                                optimizer=optimizer,\n",
    "                                dwt=dwt,\n",
    "                                dwt_len=dwt_len,\n",
    "                                thresh=thresh)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                              dataloader=test_dataloader,\n",
    "                              loss_fn=loss_fn,\n",
    "                              dwt=dwt,\n",
    "                              dwt_len=dwt_len,\n",
    "                              thresh=thresh)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1}\\t| train_loss:\\t{train_loss:.5f} | train_accuracy:\\t{train_acc:.3f} | test_loss:\\t{test_loss:.5f} | test_accuracy:\\t{test_acc:.3f}\")\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        if (epoch % save_freq == 0 and epoch != 0 and save) or (epoch == epochs-1 and save):\n",
    "            torch.save(model.state_dict(), f\"../data/CNN_{epoch}_epochs.pkl\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t| train_loss:\t2.30104 | train_accuracy:\t18.160 | test_loss:\t2.29886 | test_accuracy:\t23.193\n",
      "Epoch: 2\t| train_loss:\t2.29166 | train_accuracy:\t25.423 | test_loss:\t2.28002 | test_accuracy:\t28.285\n",
      "Epoch: 3\t| train_loss:\t2.26889 | train_accuracy:\t28.992 | test_loss:\t2.26020 | test_accuracy:\t29.563\n",
      "Epoch: 4\t| train_loss:\t2.25256 | train_accuracy:\t31.013 | test_loss:\t2.24804 | test_accuracy:\t30.681\n",
      "Epoch: 5\t| train_loss:\t2.24232 | train_accuracy:\t31.680 | test_loss:\t2.23989 | test_accuracy:\t31.879\n"
     ]
    }
   ],
   "source": [
    "# wavelet sparse model\n",
    "epochs = 5\n",
    "vec_len = 90\n",
    "thresh = 90\n",
    "dwt_model = MLP(input_shape=vec_len, hidden_units=256, output_shape=len(label_names))\n",
    "dwt_optimizer = torch.optim.SGD(params=dwt_model.parameters(), lr=0.1)\n",
    "dwt_results = train(dwt_model, train_dataloader, test_dataloader, dwt_optimizer, dwt_loss_fn, epochs, dwt=True, dwt_len=vec_len, thresh=thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t| train_loss:\t2.30144 | train_accuracy:\t17.130 | test_loss:\t2.29978 | test_accuracy:\t23.003\n",
      "Epoch: 2\t| train_loss:\t2.29418 | train_accuracy:\t25.745 | test_loss:\t2.28414 | test_accuracy:\t29.663\n",
      "Epoch: 3\t| train_loss:\t2.26909 | train_accuracy:\t31.615 | test_loss:\t2.25726 | test_accuracy:\t32.957\n",
      "Epoch: 4\t| train_loss:\t2.24845 | train_accuracy:\t33.723 | test_loss:\t2.24483 | test_accuracy:\t33.327\n",
      "Epoch: 5\t| train_loss:\t2.23999 | train_accuracy:\t33.403 | test_loss:\t2.23738 | test_accuracy:\t33.147\n"
     ]
    }
   ],
   "source": [
    "# wavelet sparse model\n",
    "epochs = 5\n",
    "vec_len = 196\n",
    "thresh = 0\n",
    "dwt_model100 = MLP(input_shape=vec_len, hidden_units=256, output_shape=len(label_names))\n",
    "dwt_optimizer100 = torch.optim.SGD(params=dwt_model100.parameters(), lr=0.1)\n",
    "dwt_results = train(dwt_model100, train_dataloader, test_dataloader, dwt_optimizer100, dwt_loss_fn, epochs, dwt=True, dwt_len=vec_len, thresh=thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t| train_loss:\t2.30129 | train_accuracy:\t17.515 | test_loss:\t2.29955 | test_accuracy:\t23.972\n",
      "Epoch: 2\t| train_loss:\t2.29483 | train_accuracy:\t26.300 | test_loss:\t2.28697 | test_accuracy:\t27.945\n",
      "Epoch: 3\t| train_loss:\t2.27283 | train_accuracy:\t29.617 | test_loss:\t2.26009 | test_accuracy:\t30.771\n",
      "Epoch: 4\t| train_loss:\t2.25307 | train_accuracy:\t30.677 | test_loss:\t2.24917 | test_accuracy:\t29.902\n",
      "Epoch: 5\t| train_loss:\t2.24515 | train_accuracy:\t30.728 | test_loss:\t2.24511 | test_accuracy:\t29.543\n"
     ]
    }
   ],
   "source": [
    "# wavelet sparse model\n",
    "epochs = 5\n",
    "vec_len = 90\n",
    "thresh = 80\n",
    "dwt_model2 = MLP(input_shape=vec_len, hidden_units=256, output_shape=len(label_names))\n",
    "dwt_optimizer2 = torch.optim.SGD(params=dwt_model2.parameters(), lr=0.1)\n",
    "dwt_results = train(dwt_model2, train_dataloader, test_dataloader, dwt_optimizer2, dwt_loss_fn, epochs, dwt=True, dwt_len=vec_len, thresh=thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t| train_loss:\t2.15524 | train_accuracy:\t63.372 | test_loss:\t2.10357 | test_accuracy:\t66.094\n",
      "Epoch: 2\t| train_loss:\t2.09134 | train_accuracy:\t68.732 | test_loss:\t2.08258 | test_accuracy:\t69.669\n",
      "Epoch: 3\t| train_loss:\t2.07890 | train_accuracy:\t71.587 | test_loss:\t2.07620 | test_accuracy:\t71.615\n",
      "Epoch: 4\t| train_loss:\t2.07479 | train_accuracy:\t72.682 | test_loss:\t2.07514 | test_accuracy:\t72.045\n",
      "Epoch: 5\t| train_loss:\t2.07279 | train_accuracy:\t73.333 | test_loss:\t2.07372 | test_accuracy:\t73.343\n",
      "Epoch: 6\t| train_loss:\t2.07074 | train_accuracy:\t74.017 | test_loss:\t2.07364 | test_accuracy:\t73.223\n",
      "Epoch: 7\t| train_loss:\t2.06991 | train_accuracy:\t74.603 | test_loss:\t2.07308 | test_accuracy:\t74.311\n",
      "Epoch: 8\t| train_loss:\t2.06887 | train_accuracy:\t75.073 | test_loss:\t2.07181 | test_accuracy:\t74.012\n",
      "Epoch: 9\t| train_loss:\t2.06854 | train_accuracy:\t75.487 | test_loss:\t2.07203 | test_accuracy:\t74.341\n",
      "Epoch: 10\t| train_loss:\t2.06771 | train_accuracy:\t75.697 | test_loss:\t2.07153 | test_accuracy:\t74.571\n"
     ]
    }
   ],
   "source": [
    "# test normal model\n",
    "epochs = 10\n",
    "normal_model = MLP(input_shape=28*28, hidden_units=512, output_shape=len(label_names))\n",
    "normal_optimizer = torch.optim.SGD(params=normal_model.parameters(), lr=0.1)\n",
    "normal_results = train(normal_model, train_dataloader, test_dataloader, normal_optimizer, normal_loss_fn, epochs, dwt=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
